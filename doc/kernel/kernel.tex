\documentclass{article}


\usepackage{natbib}
\bibliographystyle{apa}

\usepackage{amsmath}

\usepackage{url}

\newenvironment{eqnnon}{\begin{equation}}{\end{equation}}
\newenvironment{eqnarraynon}{\begin{eqnarray}}{\end{eqnarray}}

\input{kernel_symbol.tex}

\begin{document}

\title{Kernel methods: SVM versus kernel density estimation}

\author{Peter Mills}

\maketitle

\tableofcontents

\section{Introduction}

This is an introductory article on kernel density estimators and closely
related kernel classifiers.
The author would like to focus on two things that gave him confusion when he was
first learning about these topics.
The first is to show how kernel density estimators, 
such as k-nearest neighbours(KNN),
are related to support vector machines (SVM).
The second is to show how the ``kernel trick'' is used to derive the
minimization problem in SVM.

This intro was originally included as the background to \citet{Mills2018},
however the reviewers requested that it be shortened and most of it was
removed.
The material still appears in the arXiv version.

\section{Theory}

\label{theory}

\subsection{Kernel estimation}

A kernel is a scalar function of two vectors that can be used for 
non-parametric density estimation. A typical ``kernel-density estimator"
looks like this:
\begin{eqnnon}
	\densityestimator(\testpoint) = \frac {1}{\nsample \norm} \sum_{i=1}^\nsample \vectorkernel(\testpoint, \sample_i, \kernelparam)
	\label{kernel_estimator}
\end{eqnnon}
where $\densityestimator$ is an estimator for the density, $\probability$, 
$\vectorkernel$ is the kernel function,
$\lbrace \sample_i | ~ i \in [1,\nsample] \rbrace$ are a set of training samples, 
$\testpoint$ is the test point,
and $\kernelparam$ is a set of parameters. 
The normalization coefficient, $\norm$, normalizes $\vectorkernel$:
\begin{eqnnon}
	\norm = \int_V \vectorkernel(\point, \point^\prime, \kernelparam) \mathrm d \point^\prime
	\label{norm_def}
\end{eqnnon}

The method can be used for statistical classification by comparing
results from the different classes:
\begin{equation}
	\class = \arg \max_i \sum_{j|\classlabel_j = i} K(\testpoint, \sample_i, \kernelparam)
	\label{kernel_classification}
\end{equation}
where $\classlabel_j$ is the class of the $j$th sample.
Similarly, the method can also return estimates of
the joint ($\condprob(\class, \vec x)$) and conditional probabilities 
($\condprob(\class |\vec x)$)
by dividing the sum in (\ref{kernel_classification})
by $\nsample \norm$ or by the sum of all the kernels, respectively. 

If the same kernel is used for every sample and every test point, the estimator
may be sub-optimal, particularly in regions of very high or very low density.
There are at least two ways to address this problem.
In a ``variable-bandwidth'' estimator, the coefficients, $\kernelparam$, depend in some
way on the density itself. 
Since the actual density is normally unavailable, the
estimated density can be used as a proxy
\citep{Terrell_Scott1992,Mills2011}.

Let the kernel function take the following form:
\begin{eqnnon}
	\vectorkernel(\point, \point^\prime, \bandwidth) = \scalarkernel \left (\frac{|\point - \point^\prime|}{\bandwidth} \right )
	\label{scalar_kernel_def}
\end{eqnnon}
where $\bandwidth$ is the ``bandwidth''. 
In \citet{Mills2011}, $\bandwidth$ is made proportional to the density:
\begin{eqnnon}
	\bandwidth \propto \frac{1}{\probability^{1/\dimension}} \approx \frac{1}{\densityestimator^{1/\dimension}}
	\label{setting_the_bandwidth}
\end{eqnnon}
where $\dimension$ is the dimension of the feature space.
Since the normalization coefficient, $\norm$, must include the factor,
${\bandwidth^\dimension}$, some rearrangement shows that:
\begin{equation}
	\frac{1}{\nsample} \sum_i \scalarkernel \left (\frac{|\testpoint - \sample_i|}{\bandwidth} \right ) = \kernelsum = const.
	\label{agf_def}
\end{equation}
This is a generalization of the $k$-nearest-neighbours (KNN) scheme in which the
free parameter, $\kernelsum$, takes the place of $k$ \citep{Mills2009,Mills2011}.
The bandwidth is determined uniquely for a given test point but is held constant for that
one, which makes this a ``balloon'' estimator. 
Contrast a ``point-wise'' estimator
in which bandwidths are different for each training point but need only be determined once
\citep{Terrell_Scott1992}.

A common kernel is the multi-dimensional Gaussian:
\begin{equation}
	\vectorkernel(\point, \point^\prime, \bandwidth) = \exp \left ( - \frac{|\point - \point^\prime|^2}{2 \bandwidth^2} \right )
	\label{Gaussian_kernel}
\end{equation}
%$\scalarkernel(x)=\exp(-x^2/2)$
also called the normal equation and in SVM it is referred to as a
``radial basis function'' or RBF for short.
Gaussian kernels were used in \citet{Mills2009} and \citet{Mills2011} in a
classifier known as adaptive Gaussian filter (AGF)
to classify satellite data.

To return the KNN method, consider a ``square'' kernel that returns 1 below
a certain radius and 0 otherwise:
\begin{equation}
	\scalarkernel(r) = \left \lbrace
	\begin{array}{lr}
		1; & r \ge \bandwidth \\
		0; & r > \bandwidth
	\end{array} \right .
\end{equation}

Another method of improving the performance of a kernel-density estimator
is to multiply each kernel by a coefficient:
\begin{eqnnon}
	\densityestimator(\testpoint) = \sum_i \svmcoeff_i \vectorkernel(\testpoint, \sample_i, \kernelparam)
	\label{weighted_kernel_estimator}
\end{eqnnon}
The coefficients, $\lbrace \svmcoeff_i \rbrace$, are found through an optimization
procedure designed to minimize the error \citep{Chen_etal2015}. In the most popular
form of this kernel method, support vector machines (SVM), the coefficients
are the result of a complex, dual optimization procedure which minimizes
the classification error. We will briefly outline this procedure.

\subsection{Support Vector Machines}

The basic ``trick'' of kernel-based SVM methods is to replace a dot product
with the kernel function in the assumption that it can be rewritten
as a dot product of a transformed and expanded feature space:
\begin{eqnnon}
	\vectorkernel(\point, \point^\prime) = \expandedspace(\point) \cdot \expandedspace(\point^\prime)
	\label{phi_def}
\end{eqnnon}
For simplicity we have ommitted the kernel parameters.
$\expandedspace$ is a vector function of the feature space.
The simplest example of a kernel function that has a closed, analytical and
finite-dimensional $\expandedspace$ is the square of the dot product:
\begin{eqnarray}
	\vectorkernel(\point, \point^\prime) & = & (\point \cdot \point^\prime)^2 \\ \nonumber
					 & = & (\coord_1^2, \coord_2^2, \coord_3^2, ..., \sqrt{2} \coord_1 \coord_2, \sqrt{2} \coord_1 \coord_3, ... \sqrt{2} \coord_2 \coord_3, ...) \cdot \\ \nonumber
      & &	 ({\coord^\prime_1}^2, {\coord^\prime}_2^2, {\coord^\prime}_3^2, ..., \sqrt{2} \coord^\prime_1 \coord^\prime_2, \sqrt{2} \coord^\prime_1 \coord^\prime_3, ... \sqrt{2} \coord^\prime_2 \coord^\prime_3, ...) 
\end{eqnarray}
but it should be noted that in more complex cases, 
there is no need to actually construct $\expandedspace$ since it is replaced by the 
kernel function, $\vectorkernel$, in the final analysis.

In a binary SVM classifier, the classes are separated by a single hyper-plane
defined by $\svmbordernormal$ and $\svmborderconst$.
In a kernel-based SVM, this hyperplane bisects not the regular feature
space, but the theoretical, transformed space defined by the function,
$\expandedspace(\point)$.
The decision value is calculated via a dot product:
\begin{equation}
	\svmdecision(\testpoint)=\svmbordernormal \cdot \expandedspace(\testpoint) + \svmborderconst
	\label{decision_function0}
\end{equation}
and the class determined, as before, by the sign of the decision value:
\begin{equation}
	\class(\testpoint) = \frac{\svmdecision(\testpoint)}{|\svmdecision(\testpoint)|}
	\label{class_value}
\end{equation}
where for convenience, the class labels are given by $c \in \lbrace -1, 1 \rbrace$.

In the first step of the minimization procedure, 
the magnitude of the border normal, $\svmbordernormal$, 
is minimized subject to the constraint that there are no classification 
errors:
\begin{eqnarray*}
	\min_{\svmbordernormal, \svmborderconst} \frac{1}{2} | \svmbordernormal | \\ \nonumber
	\svmdecision(\sample_i) \classlabel_i \ge 1
\end{eqnarray*}
Introducing the coefficients, $\lbrace \svmcoeff_i \rbrace$, 
as Lagrange multipliers on the constraints:
\begin{equation}
	\min_{\svmbordernormal, b} \left \lbrace \frac{1}{2} | \svmbordernormal | - \sum_i \svmcoeff_i \left [ \svmdecision(\sample_i) \classlabel_i -1 \right ] \right \rbrace
\end{equation}
generates the following pair of analytic expressions:
\begin{eqnarray}
	\sum_i \svmcoeff_i \classlabel_i & = & 0 \\
	\svmbordernormal & = & \sum_i \svmcoeff_i \classlabel \expandedspace(\sample_i) \label{border_vector_equation}
\end{eqnarray}
through setting the derivatives w.r.t. the minimizers to zero.
Substituting the second equation, (\ref{border_vector_equation}),
into the decision function in (\ref{decision_function0}) produces the following:
\begin{equation}
	\svmdecision(\testpoint) = \sum_i \svmcoeff_i \classlabel_i \vectorkernel (\testpoint, \sample_i) + \svmborderconst
	\label{svm_decision}
\end{equation}
Thus, the final, dual, quadratic optimization problem looks like this:
\begin{equation}
	\max_{\lbrace \svmcoeff_i \rbrace} \left [ \sum_i \svmcoeff_i 
	- \frac{1}{2} \sum_{i, j} \svmcoeff_i \svmcoeff_j \classlabel_i \classlabel_j \vectorkernel(\sample_i, \sample_j) \right ] \label{dual_problem}
\end{equation}
\begin{eqnarray}
	\svmcoeff_i & \ge & 0 \label{constraint1} \\
	\sum_i \svmcoeff_i \classlabel_i & = & 0 \label{constraint2}
\end{eqnarray}
There are a number of refinements that can be applied to the optimization
problem in (\ref{dual_problem})-(\ref{constraint2}), chiefly to reduce over-fitting and to add
some ``margin'' to the decision border to allow for the possibility of
classification errors.
For instance, substitute the following for (\ref{constraint1}):
\begin{equation}
 0 \le \svmcoeff_i \le \svmcost
 \label{svmcost}
\end{equation}
where $\svmcost$ is the cost \citep{Mueller_etal2001}.

Three things should be noted. First, the function $\expandedspace$ appears in
neither the final decision function, (\ref{svm_decision}), nor in the
optimization problem, (\ref{dual_problem}). Second, while the use of
$\expandedspace$ implies that the time complexity of the decision function 
could be $O(1)$ as in a parametric statistical model, in actual fact it is
dependent on the number of non-zero values in $\lbrace \svmcoeff_i \rbrace$.
While the coefficient set, $\lbrace \svmcoeff_i \rbrace$, does tend to be sparse,
nonetheless in most real problems the number of non-zero coefficients is
proportional to the number of samples, $\nsample$, producing a time complexity
of $O(\nsample)$. Thus for large problems, calculating the decision value will
be slow, just as in other kernel estimation problems.

Third, since SVM is a linear classifier operating in a transformed and expanded
feature space, using a simple dot product for the kernel funcion returns a linear classifier:
\begin{eqnarray}
	\vectorkernel(\point, \point^\prime) & = & \point \cdot \point^\prime \\
	\svmdecision(\testpoint) & = & \sum_i \svmcoeff_i \classlabel_i \testpoint \cdot \sample_i + \svmborderconst
\end{eqnarray}
By pulling out the test point from the summation we can show that the border
to the class normal is equal to:
\begin{equation}
	\svmbordernormal = \sum_i \svmcoeff_i \classlabel_i \sample_i
\end{equation}
while the decision function is calculated:
\begin{equation}
	\svmdecision(\testpoint)=\svmbordernormal \cdot \testpoint + \svmborderconst
\end{equation}
This is a {\it linear classifier} or {\it perceptron}.
	
\section{Software}

\label{methods}

\subsection{LIBSVM}

A good software library for support vector machines is LIBSVM. 
It was developed by Chih-Chung Chang and Chih-Jen Lin of 
the National Taiwan University, Taipei, Taiwan \citep{Chang_Lin2011}.
LIBSVM can be found at: \url{https://www.csie.ntu.edu.tw/~cjlin/libsvm}

\subsection{LibAGF}

libAGF is another software library with programes for performing kernel
density ``balloon'' estimation using Gaussian kernels and $k$-nearest-neighbours\citep{Mills2011}.
It was written by Peter Mills and can be found at
\url{https://github.com/peteysoft/libmsci}.

\bibliography{../ML_learning}

\end{document}


